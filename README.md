# ü§ñ Generador de Productos con IA

Una aplicaci√≥n completa que utiliza inteligencia artificial para generar contenido comercial para productos: descripciones atractivas, im√°genes profesionales y comentarios de clientes realistas.

## üöÄ Caracter√≠sticas

- **üìù Generaci√≥n de Descripciones**: Crea t√≠tulos y descripciones comerciales optimizadas
- **üñºÔ∏è Generaci√≥n de Im√°genes**: Produce im√°genes profesionales usando Stable Diffusion
- **üí¨ Comentarios de Productos**: Genera comentarios realistas de clientes y analiza opiniones existentes
- **üê≥ Despliegue con Docker**: Configuraci√≥n completa con Docker Compose y soporte GPU
- **‚ö° Frontend Moderno**: Interfaz React con Vite, responsive y f√°cil de usar
- **üîå API RESTful**: Endpoints documentados y listos para integraci√≥n

## üõ†Ô∏è Stack Tecnol√≥gico

### Backend

- **Node.js 22** con ES Modules
- **Express.js** para la API REST
- **Ollama** para modelos de IA locales
- **Python** para generaci√≥n de im√°genes (Stable Diffusion)
- **Zod** para validaci√≥n de datos

### Frontend

- **React 18** sin TypeScript
- **Vite** como bundler
- **React Router** para navegaci√≥n
- **Axios** para peticiones HTTP
- **Lucide React** para iconos
- **React Hot Toast** para notificaciones

### DevOps

- **Docker** y **Docker Compose**
- **Nginx** como proxy reverso
- **Soporte GPU** para NVIDIA
- **Health checks** integrados

## üìã Requisitos del Sistema

- **Docker** y **Docker Compose** instalados
- **NVIDIA Docker runtime** (para soporte GPU opcional)
- **8GB RAM m√≠nimo** (16GB recomendado)
- **10GB de espacio libre** (para modelos de IA)

## üîß Instalaci√≥n y Configuraci√≥n

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd ollama-node-docker
```

### 2. Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar configuraci√≥n (opcional)
nano .env
```

### 3. Despliegue con Docker Compose

#### Opci√≥n A: Con GPU (Recomendado)

```bash
# Verificar soporte GPU
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# Ejecutar con GPU
docker-compose up -d
```

#### Opci√≥n B: Solo CPU

```bash
# Editar docker-compose.yml y comentar la secci√≥n GPU
# Luego ejecutar:
docker-compose up -d
```

### 4. Instalar Modelo de IA

```bash
# Acceder al contenedor de Ollama
docker exec -it ollama_service ollama pull gemma3:1b

# O usar otro modelo (opcional)
docker exec -it ollama_service ollama pull llama3:8b
```

### 5. Verificar Instalaci√≥n

- **Frontend**: http://localhost
- **API Backend**: http://localhost:3000
- **Documentaci√≥n API**: http://localhost/docs
- **Ollama**: http://localhost:11434

## üíª Desarrollo Local

### Backend

```bash
# Instalar dependencias
npm install

# Ejecutar en modo desarrollo
npm run dev

# Ejecutar en producci√≥n
npm start
```

### Frontend

```bash
cd frontend

# Instalar dependencias
npm install

# Ejecutar servidor de desarrollo
npm run dev

# Construir para producci√≥n
npm run build
```

## üìñ Uso de la Aplicaci√≥n

### 1. Generaci√≥n de Descripciones

1. Ve a la secci√≥n "Descripciones"
2. Ingresa el nombre y detalles del producto
3. Haz clic en "Generar Descripciones"
4. Copia las propuestas generadas

### 2. Generaci√≥n de Im√°genes

1. Ve a la secci√≥n "Im√°genes"
2. Completa la informaci√≥n del producto
3. Selecciona estilo, variaciones y dimensiones
4. Genera y descarga las im√°genes

### 3. Comentarios de Productos

#### Generar Comentarios

1. Ve a la pesta√±a "Generar Comentarios"
2. Configura producto, n√∫mero y sentimiento
3. Genera comentarios realistas

#### Resumir Comentarios

1. Ve a la pesta√±a "Resumir Comentarios"
2. Pega comentarios existentes (uno por l√≠nea)
3. Obt√©n an√°lisis detallado con insights

## üîå API Endpoints

### Descripciones de Productos

```
POST /api/chat/generate-detailed-product-info
```

### Im√°genes

```
POST /api/images/generate
GET  /api/images/serve/:filename
GET  /api/images/download/:filename
GET  /api/images/list
```

### Comentarios

```
POST /api/comments/generate
POST /api/comments/summarize
```

Ver documentaci√≥n completa en: http://localhost/docs

## üê≥ Comandos Docker √ötiles

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio espec√≠fico
docker-compose logs -f backend

# Reiniciar servicios
docker-compose restart

# Detener todo
docker-compose down

# Limpiar vol√∫menes (¬°cuidado!)
docker-compose down -v

# Ver estado de los servicios
docker-compose ps
```

## üîß Configuraci√≥n Avanzada

### Cambiar Modelo de IA

```bash
# Listar modelos disponibles
docker exec -it ollama_service ollama list

# Instalar nuevo modelo
docker exec -it ollama_service ollama pull llama3:8b

# Actualizar variable de entorno
MODEL_NAME=llama3:8b
```

### Configurar GPU

```bash
# Verificar drivers NVIDIA
nvidia-smi

# Instalar NVIDIA Container Runtime
curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list
sudo apt-get update
sudo apt-get install nvidia-container-runtime
```

## üö® Soluci√≥n de Problemas

### Error: Puerto ocupado

```bash
# Verificar qu√© proceso usa el puerto
sudo lsof -i :3000
sudo lsof -i :80

# Cambiar puerto en .env
API_PORT=3001
```

### Error: GPU no detectada

```bash
# Verificar NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# Si falla, usar versi√≥n CPU
# Comentar secci√≥n GPU en docker-compose.yml
```

### Error: Modelo no encontrado

```bash
# Verificar modelos instalados
docker exec -it ollama_service ollama list

# Instalar modelo necesario
docker exec -it ollama_service ollama pull gemma3:1b
```

### Limpiar cache y reiniciar

```bash
# Detener servicios
docker-compose down

# Limpiar im√°genes Docker
docker system prune -f

# Reconstruir y ejecutar
docker-compose up --build -d
```

## üìä Monitoreo y Logs

### Ver Estado de Servicios

```bash
# Estado general
docker-compose ps

# Health checks
docker-compose exec backend curl -f http://localhost:3000/api/images/service/status
```

### Logs Detallados

```bash
# Todos los logs
docker-compose logs -f

# Solo backend
docker-compose logs -f backend

# Solo frontend
docker-compose logs -f frontend

# Solo Ollama
docker-compose logs -f ollama
```

## ü§ù Contribuci√≥n

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia ISC. Ver `package.json` para m√°s detalles.

## üÜò Soporte

Si encuentras problemas o tienes preguntas:

1. Revisa la secci√≥n de soluci√≥n de problemas
2. Verifica los logs con `docker-compose logs -f`
3. Abre un issue en el repositorio

## üîÑ Actualizaciones

Para actualizar la aplicaci√≥n:

```bash
# Obtener √∫ltimos cambios
git pull origin main

# Reconstruir y ejecutar
docker-compose up --build -d

# Actualizar modelos de IA si es necesario
docker exec -it ollama_service ollama pull gemma3:1b
```

---

‚≠ê **¬°No olvides dar una estrella al repositorio si te ha sido √∫til!** ‚≠ê
